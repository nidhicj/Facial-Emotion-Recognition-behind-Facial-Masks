DB: R
Device state: cuda

FER on RAF-DB using GACNN


Total included  9796 {0: 2048, 1: 3854, 2: 549, 3: 1047, 4: 1546, 5: 207, 6: 545}
Total included  3062 {0: 655, 1: 1140, 2: 178, 3: 314, 4: 513, 5: 90, 6: 172}
Total included  2449 {0: 494, 1: 959, 2: 137, 3: 256, 4: 385, 5: 58, 6: 160}
length of  train Database for training: 9796
length of  valid Database for validation training: 3062
length of  test Database: 2449
prepare model
+------------------------------------------+------------+
|                 Modules                  | Parameters |
+------------------------------------------+------------+
|           module.base.0.weight           |    1728    |
|            module.base.0.bias            |     64     |
|           module.base.2.weight           |   36864    |
|            module.base.2.bias            |     64     |
|           module.base.5.weight           |   73728    |
|            module.base.5.bias            |    128     |
|           module.base.7.weight           |   147456   |
|            module.base.7.bias            |    128     |
|          module.base.10.weight           |   294912   |
|           module.base.10.bias            |    256     |
|          module.base.12.weight           |   589824   |
|           module.base.12.bias            |    256     |
|          module.base.14.weight           |   589824   |
|           module.base.14.bias            |    256     |
|          module.base.17.weight           |  1179648   |
|           module.base.17.bias            |    512     |
|          module.base.19.weight           |  2359296   |
|           module.base.19.bias            |    512     |
| module.local_attention_block.0.1.weight  |   65536    |
|  module.local_attention_block.0.1.bias   |    128     |
| module.local_attention_block.0.3.weight  |    128     |
|  module.local_attention_block.0.3.bias   |    128     |
| module.local_attention_block.0.5.weight  |   73728    |
|  module.local_attention_block.0.5.bias   |     64     |
| module.local_attention_block.0.7.weight  |     64     |
|  module.local_attention_block.0.7.bias   |     1      |
| module.local_attention_block.1.1.weight  |   65536    |
|  module.local_attention_block.1.1.bias   |    128     |
| module.local_attention_block.1.3.weight  |    128     |
|  module.local_attention_block.1.3.bias   |    128     |
| module.local_attention_block.1.5.weight  |   73728    |
|  module.local_attention_block.1.5.bias   |     64     |
| module.local_attention_block.1.7.weight  |     64     |
|  module.local_attention_block.1.7.bias   |     1      |
| module.local_attention_block.2.1.weight  |   65536    |
|  module.local_attention_block.2.1.bias   |    128     |
| module.local_attention_block.2.3.weight  |    128     |
|  module.local_attention_block.2.3.bias   |    128     |
| module.local_attention_block.2.5.weight  |   73728    |
|  module.local_attention_block.2.5.bias   |     64     |
| module.local_attention_block.2.7.weight  |     64     |
|  module.local_attention_block.2.7.bias   |     1      |
| module.local_attention_block.3.1.weight  |   65536    |
|  module.local_attention_block.3.1.bias   |    128     |
| module.local_attention_block.3.3.weight  |    128     |
|  module.local_attention_block.3.3.bias   |    128     |
| module.local_attention_block.3.5.weight  |   73728    |
|  module.local_attention_block.3.5.bias   |     64     |
| module.local_attention_block.3.7.weight  |     64     |
|  module.local_attention_block.3.7.bias   |     1      |
| module.local_attention_block.4.1.weight  |   65536    |
|  module.local_attention_block.4.1.bias   |    128     |
| module.local_attention_block.4.3.weight  |    128     |
|  module.local_attention_block.4.3.bias   |    128     |
| module.local_attention_block.4.5.weight  |   73728    |
|  module.local_attention_block.4.5.bias   |     64     |
| module.local_attention_block.4.7.weight  |     64     |
|  module.local_attention_block.4.7.bias   |     1      |
| module.local_attention_block.5.1.weight  |   65536    |
|  module.local_attention_block.5.1.bias   |    128     |
| module.local_attention_block.5.3.weight  |    128     |
|  module.local_attention_block.5.3.bias   |    128     |
| module.local_attention_block.5.5.weight  |   73728    |
|  module.local_attention_block.5.5.bias   |     64     |
| module.local_attention_block.5.7.weight  |     64     |
|  module.local_attention_block.5.7.bias   |     1      |
| module.local_attention_block.6.1.weight  |   65536    |
|  module.local_attention_block.6.1.bias   |    128     |
| module.local_attention_block.6.3.weight  |    128     |
|  module.local_attention_block.6.3.bias   |    128     |
| module.local_attention_block.6.5.weight  |   73728    |
|  module.local_attention_block.6.5.bias   |     64     |
| module.local_attention_block.6.7.weight  |     64     |
|  module.local_attention_block.6.7.bias   |     1      |
| module.local_attention_block.7.1.weight  |   65536    |
|  module.local_attention_block.7.1.bias   |    128     |
| module.local_attention_block.7.3.weight  |    128     |
|  module.local_attention_block.7.3.bias   |    128     |
| module.local_attention_block.7.5.weight  |   73728    |
|  module.local_attention_block.7.5.bias   |     64     |
| module.local_attention_block.7.7.weight  |     64     |
|  module.local_attention_block.7.7.bias   |     1      |
| module.local_attention_block.8.1.weight  |   65536    |
|  module.local_attention_block.8.1.bias   |    128     |
| module.local_attention_block.8.3.weight  |    128     |
|  module.local_attention_block.8.3.bias   |    128     |
| module.local_attention_block.8.5.weight  |   73728    |
|  module.local_attention_block.8.5.bias   |     64     |
| module.local_attention_block.8.7.weight  |     64     |
|  module.local_attention_block.8.7.bias   |     1      |
| module.local_attention_block.9.1.weight  |   65536    |
|  module.local_attention_block.9.1.bias   |    128     |
| module.local_attention_block.9.3.weight  |    128     |
|  module.local_attention_block.9.3.bias   |    128     |
| module.local_attention_block.9.5.weight  |   73728    |
|  module.local_attention_block.9.5.bias   |     64     |
| module.local_attention_block.9.7.weight  |     64     |
|  module.local_attention_block.9.7.bias   |     1      |
| module.local_attention_block.10.1.weight |   65536    |
|  module.local_attention_block.10.1.bias  |    128     |
| module.local_attention_block.10.3.weight |    128     |
|  module.local_attention_block.10.3.bias  |    128     |
| module.local_attention_block.10.5.weight |   73728    |
|  module.local_attention_block.10.5.bias  |     64     |
| module.local_attention_block.10.7.weight |     64     |
|  module.local_attention_block.10.7.bias  |     1      |
| module.local_attention_block.11.1.weight |   65536    |
|  module.local_attention_block.11.1.bias  |    128     |
| module.local_attention_block.11.3.weight |    128     |
|  module.local_attention_block.11.3.bias  |    128     |
| module.local_attention_block.11.5.weight |   73728    |
|  module.local_attention_block.11.5.bias  |     64     |
| module.local_attention_block.11.7.weight |     64     |
|  module.local_attention_block.11.7.bias  |     1      |
| module.local_attention_block.12.1.weight |   65536    |
|  module.local_attention_block.12.1.bias  |    128     |
| module.local_attention_block.12.3.weight |    128     |
|  module.local_attention_block.12.3.bias  |    128     |
| module.local_attention_block.12.5.weight |   73728    |
|  module.local_attention_block.12.5.bias  |     64     |
| module.local_attention_block.12.7.weight |     64     |
|  module.local_attention_block.12.7.bias  |     1      |
| module.local_attention_block.13.1.weight |   65536    |
|  module.local_attention_block.13.1.bias  |    128     |
| module.local_attention_block.13.3.weight |    128     |
|  module.local_attention_block.13.3.bias  |    128     |
| module.local_attention_block.13.5.weight |   73728    |
|  module.local_attention_block.13.5.bias  |     64     |
| module.local_attention_block.13.7.weight |     64     |
|  module.local_attention_block.13.7.bias  |     1      |
| module.local_attention_block.14.1.weight |   65536    |
|  module.local_attention_block.14.1.bias  |    128     |
| module.local_attention_block.14.3.weight |    128     |
|  module.local_attention_block.14.3.bias  |    128     |
| module.local_attention_block.14.5.weight |   73728    |
|  module.local_attention_block.14.5.bias  |     64     |
| module.local_attention_block.14.7.weight |     64     |
|  module.local_attention_block.14.7.bias  |     1      |
| module.local_attention_block.15.1.weight |   65536    |
|  module.local_attention_block.15.1.bias  |    128     |
| module.local_attention_block.15.3.weight |    128     |
|  module.local_attention_block.15.3.bias  |    128     |
| module.local_attention_block.15.5.weight |   73728    |
|  module.local_attention_block.15.5.bias  |     64     |
| module.local_attention_block.15.7.weight |     64     |
|  module.local_attention_block.15.7.bias  |     1      |
| module.local_attention_block.16.1.weight |   65536    |
|  module.local_attention_block.16.1.bias  |    128     |
| module.local_attention_block.16.3.weight |    128     |
|  module.local_attention_block.16.3.bias  |    128     |
| module.local_attention_block.16.5.weight |   73728    |
|  module.local_attention_block.16.5.bias  |     64     |
| module.local_attention_block.16.7.weight |     64     |
|  module.local_attention_block.16.7.bias  |     1      |
| module.local_attention_block.17.1.weight |   65536    |
|  module.local_attention_block.17.1.bias  |    128     |
| module.local_attention_block.17.3.weight |    128     |
|  module.local_attention_block.17.3.bias  |    128     |
| module.local_attention_block.17.5.weight |   73728    |
|  module.local_attention_block.17.5.bias  |     64     |
| module.local_attention_block.17.7.weight |     64     |
|  module.local_attention_block.17.7.bias  |     1      |
| module.local_attention_block.18.1.weight |   65536    |
|  module.local_attention_block.18.1.bias  |    128     |
| module.local_attention_block.18.3.weight |    128     |
|  module.local_attention_block.18.3.bias  |    128     |
| module.local_attention_block.18.5.weight |   73728    |
|  module.local_attention_block.18.5.bias  |     64     |
| module.local_attention_block.18.7.weight |     64     |
|  module.local_attention_block.18.7.bias  |     1      |
| module.local_attention_block.19.1.weight |   65536    |
|  module.local_attention_block.19.1.bias  |    128     |
| module.local_attention_block.19.3.weight |    128     |
|  module.local_attention_block.19.3.bias  |    128     |
| module.local_attention_block.19.5.weight |   73728    |
|  module.local_attention_block.19.5.bias  |     64     |
| module.local_attention_block.19.7.weight |     64     |
|  module.local_attention_block.19.7.bias  |     1      |
|  module.global_attention_block.1.weight  |   65536    |
|   module.global_attention_block.1.bias   |    128     |
|  module.global_attention_block.3.weight  |    128     |
|   module.global_attention_block.3.bias   |    128     |
|  module.global_attention_block.5.weight  |   401408   |
|   module.global_attention_block.5.bias   |     64     |
|  module.global_attention_block.7.weight  |     64     |
|   module.global_attention_block.7.bias   |     1      |
|       module.PG_unit_1.0.0.weight        |  2359296   |
|        module.PG_unit_1.0.0.bias         |    512     |
|       module.PG_unit_1.1.0.weight        |  2359296   |
|        module.PG_unit_1.1.0.bias         |    512     |
|       module.PG_unit_1.2.0.weight        |  2359296   |
|        module.PG_unit_1.2.0.bias         |    512     |
|       module.PG_unit_1.3.0.weight        |  2359296   |
|        module.PG_unit_1.3.0.bias         |    512     |
|       module.PG_unit_1.4.0.weight        |  2359296   |
|        module.PG_unit_1.4.0.bias         |    512     |
|       module.PG_unit_1.5.0.weight        |  2359296   |
|        module.PG_unit_1.5.0.bias         |    512     |
|       module.PG_unit_1.6.0.weight        |  2359296   |
|        module.PG_unit_1.6.0.bias         |    512     |
|       module.PG_unit_1.7.0.weight        |  2359296   |
|        module.PG_unit_1.7.0.bias         |    512     |
|       module.PG_unit_1.8.0.weight        |  2359296   |
|        module.PG_unit_1.8.0.bias         |    512     |
|       module.PG_unit_1.9.0.weight        |  2359296   |
|        module.PG_unit_1.9.0.bias         |    512     |
|       module.PG_unit_1.10.0.weight       |  2359296   |
|        module.PG_unit_1.10.0.bias        |    512     |
|       module.PG_unit_1.11.0.weight       |  2359296   |
|        module.PG_unit_1.11.0.bias        |    512     |
|       module.PG_unit_1.12.0.weight       |  2359296   |
|        module.PG_unit_1.12.0.bias        |    512     |
|       module.PG_unit_1.13.0.weight       |  2359296   |
|        module.PG_unit_1.13.0.bias        |    512     |
|       module.PG_unit_1.14.0.weight       |  2359296   |
|        module.PG_unit_1.14.0.bias        |    512     |
|       module.PG_unit_1.15.0.weight       |  2359296   |
|        module.PG_unit_1.15.0.bias        |    512     |
|       module.PG_unit_1.16.0.weight       |  2359296   |
|        module.PG_unit_1.16.0.bias        |    512     |
|       module.PG_unit_1.17.0.weight       |  2359296   |
|        module.PG_unit_1.17.0.bias        |    512     |
|       module.PG_unit_1.18.0.weight       |  2359296   |
|        module.PG_unit_1.18.0.bias        |    512     |
|       module.PG_unit_1.19.0.weight       |  2359296   |
|        module.PG_unit_1.19.0.bias        |    512     |
|       module.PG_unit_2.0.1.weight        |  1179648   |
|        module.PG_unit_2.0.1.bias         |     64     |
|       module.PG_unit_2.1.1.weight        |  1179648   |
|        module.PG_unit_2.1.1.bias         |     64     |
|       module.PG_unit_2.2.1.weight        |  1179648   |
|        module.PG_unit_2.2.1.bias         |     64     |
|       module.PG_unit_2.3.1.weight        |  1179648   |
|        module.PG_unit_2.3.1.bias         |     64     |
|       module.PG_unit_2.4.1.weight        |  1179648   |
|        module.PG_unit_2.4.1.bias         |     64     |
|       module.PG_unit_2.5.1.weight        |  1179648   |
|        module.PG_unit_2.5.1.bias         |     64     |
|       module.PG_unit_2.6.1.weight        |  1179648   |
|        module.PG_unit_2.6.1.bias         |     64     |
|       module.PG_unit_2.7.1.weight        |  1179648   |
|        module.PG_unit_2.7.1.bias         |     64     |
|       module.PG_unit_2.8.1.weight        |  1179648   |
|        module.PG_unit_2.8.1.bias         |     64     |
|       module.PG_unit_2.9.1.weight        |  1179648   |
|        module.PG_unit_2.9.1.bias         |     64     |
|       module.PG_unit_2.10.1.weight       |  1179648   |
|        module.PG_unit_2.10.1.bias        |     64     |
|       module.PG_unit_2.11.1.weight       |  1179648   |
|        module.PG_unit_2.11.1.bias        |     64     |
|       module.PG_unit_2.12.1.weight       |  1179648   |
|        module.PG_unit_2.12.1.bias        |     64     |
|       module.PG_unit_2.13.1.weight       |  1179648   |
|        module.PG_unit_2.13.1.bias        |     64     |
|       module.PG_unit_2.14.1.weight       |  1179648   |
|        module.PG_unit_2.14.1.bias        |     64     |
|       module.PG_unit_2.15.1.weight       |  1179648   |
|        module.PG_unit_2.15.1.bias        |     64     |
|       module.PG_unit_2.16.1.weight       |  1179648   |
|        module.PG_unit_2.16.1.bias        |     64     |
|       module.PG_unit_2.17.1.weight       |  1179648   |
|        module.PG_unit_2.17.1.bias        |     64     |
|       module.PG_unit_2.18.1.weight       |  1179648   |
|        module.PG_unit_2.18.1.bias        |     64     |
|       module.PG_unit_2.19.1.weight       |  1179648   |
|        module.PG_unit_2.19.1.bias        |     64     |
|        module.GG_unit_1.1.weight         |  2359296   |
|         module.GG_unit_1.1.bias          |    512     |
|        module.GG_unit_2.1.weight         |  51380224  |
|         module.GG_unit_2.1.bias          |    512     |
|            module.fc1.weight             |   917504   |
|             module.fc1.bias              |    512     |
|            module.fc2.weight             |    3584    |
|             module.fc2.bias              |     7      |
+------------------------------------------+------------+
Total Trainable Params: 133991004
Training starting:

Training Epoch: [0][0/979]	Time  (5.24655818939209)	Data (1.5372631549835205)	loss  (1.9304183721542358)	Prec1  (10.0) 	
Training Epoch: [0][100/979]	Time  (0.572925881584092)	Data (0.016039664202397413)	loss  (1.9239322501834075)	Prec1  (18.712871551513672) 	
Training Epoch: [0][200/979]	Time  (0.5355117605693305)	Data (0.008222799396040427)	loss  (1.8335772337605112)	Prec1  (24.87562370300293) 	
Training Epoch: [0][300/979]	Time  (0.5235338876413744)	Data (0.005623279615890148)	loss  (1.7193982654631732)	Prec1  (30.1993350982666) 	
Training Epoch: [0][400/979]	Time  (0.5181707658078011)	Data (0.004310873083937495)	loss  (1.6111916149792231)	Prec1  (35.3366584777832) 	
Training Epoch: [0][500/979]	Time  (0.5142099695529291)	Data (0.003525250448200279)	loss  (1.53277235134633)	Prec1  (39.32135772705078) 	
Training Epoch: [0][600/979]	Time  (0.51192184018216)	Data (0.0029969508953380107)	loss  (1.4458513592126563)	Prec1  (43.16139602661133) 	
Training Epoch: [0][700/979]	Time  (0.5190352570483416)	Data (0.0026241760281115216)	loss  (1.3743240288677978)	Prec1  (46.60485076904297) 	
Training Epoch: [0][800/979]	Time  (0.545287568619784)	Data (0.0023502398668305852)	loss  (1.3105812943383548)	Prec1  (49.43820571899414) 	
Training Epoch: [0][900/979]	Time  (0.5657481612693457)	Data (0.0021363064134028325)	loss  (1.242787230391878)	Prec1  (52.34184265136719) 	
The current loss: 342
The Last loss:  1000

******************************
	Adjusted learning rate: 1

0.00095
Training Epoch: [1][0/979]	Time  (2.2198238372802734)	Data (1.5357320308685303)	loss  (0.4069240093231201)	Prec1  (80.0) 	
Training Epoch: [1][100/979]	Time  (0.7445079879005356)	Data (0.015620927999515345)	loss  (0.6218939625685758)	Prec1  (79.1089096069336) 	
Training Epoch: [1][200/979]	Time  (0.7375818781591766)	Data (0.008065403990484589)	loss  (0.6307729723960606)	Prec1  (77.26368713378906) 	
Training Epoch: [1][300/979]	Time  (0.7261092599444215)	Data (0.005522430141106792)	loss  (0.6057087015174948)	Prec1  (78.20597839355469) 	
Training Epoch: [1][400/979]	Time  (0.6890774314243002)	Data (0.004245274084761851)	loss  (0.5997584912916669)	Prec1  (78.65336608886719) 	
Training Epoch: [1][500/979]	Time  (0.6900503959008557)	Data (0.003468640550167975)	loss  (0.571935182887161)	Prec1  (79.7205581665039) 	
Training Epoch: [1][600/979]	Time  (0.6967855257519866)	Data (0.0029687056327223184)	loss  (0.5544182019069568)	Prec1  (80.39933013916016) 	
Training Epoch: [1][700/979]	Time  (0.7015207218545649)	Data (0.0026093083679591027)	loss  (0.5419405935738719)	Prec1  (80.98431396484375) 	
Training Epoch: [1][800/979]	Time  (0.6976674793662501)	Data (0.0023379281219025228)	loss  (0.5329711753409937)	Prec1  (81.18601989746094) 	
Training Epoch: [1][900/979]	Time  (0.6911801138146472)	Data (0.0021169267669237414)	loss  (0.5207673834944836)	Prec1  (81.59822845458984) 	
The current loss: 357
The Last loss:  342
trigger times: 1

******************************
	Adjusted learning rate: 2

0.0009025
Training Epoch: [2][0/979]	Time  (2.174207925796509)	Data (1.5472791194915771)	loss  (0.6084517240524292)	Prec1  (70.0) 	
Training Epoch: [2][100/979]	Time  (0.5949992024072326)	Data (0.015645385968803178)	loss  (0.3949575389523317)	Prec1  (87.5247573852539) 	
Training Epoch: [2][200/979]	Time  (0.6565855083180897)	Data (0.008037046413516524)	loss  (0.36787666622271287)	Prec1  (87.91045379638672) 	
Training Epoch: [2][300/979]	Time  (0.6746456282479423)	Data (0.005514514010609978)	loss  (0.3457525448846179)	Prec1  (88.67108917236328) 	
Training Epoch: [2][400/979]	Time  (0.6882047605633438)	Data (0.004258297328045243)	loss  (0.34633022753775533)	Prec1  (88.65336608886719) 	
Training Epoch: [2][500/979]	Time  (0.6903139911963793)	Data (0.0034931086732479864)	loss  (0.32753634468307014)	Prec1  (89.261474609375) 	
Training Epoch: [2][600/979]	Time  (0.6852112752625629)	Data (0.00298021121350382)	loss  (0.3265778738297344)	Prec1  (89.41763305664062) 	
Training Epoch: [2][700/979]	Time  (0.6923281098908602)	Data (0.002630185468730845)	loss  (0.32168338083293085)	Prec1  (89.54351043701172) 	
Training Epoch: [2][800/979]	Time  (0.6975520701890581)	Data (0.002370484015170703)	loss  (0.3110757350248573)	Prec1  (89.85018920898438) 	
Training Epoch: [2][900/979]	Time  (0.7019578764891121)	Data (0.002170188577802279)	loss  (0.3028069185845675)	Prec1  (90.15538024902344) 	
The current loss: 304
The Last loss:  357

******************************
	Adjusted learning rate: 3

0.000857375
Training Epoch: [3][0/979]	Time  (2.5399584770202637)	Data (1.9046406745910645)	loss  (0.12420624494552612)	Prec1  (100.0) 	
Training Epoch: [3][100/979]	Time  (0.7665781361041683)	Data (0.019348541108688506)	loss  (0.2594941419706044)	Prec1  (91.2871322631836) 	
Training Epoch: [3][200/979]	Time  (0.7417082217202258)	Data (0.00999191625794368)	loss  (0.23575445408678025)	Prec1  (92.6368179321289) 	
Training Epoch: [3][300/979]	Time  (0.6896771418295825)	Data (0.006824228850710035)	loss  (0.23763345571797947)	Prec1  (92.2923583984375) 	
Training Epoch: [3][400/979]	Time  (0.7012265846319032)	Data (0.005258435917614107)	loss  (0.22555953456539465)	Prec1  (92.61845397949219) 	
Training Epoch: [3][500/979]	Time  (0.7081235773311165)	Data (0.004314398337267116)	loss  (0.23742633512217604)	Prec1  (92.35528564453125) 	
Training Epoch: [3][600/979]	Time  (0.7127516833002278)	Data (0.003694735430243011)	loss  (0.23001262427969354)	Prec1  (92.5457534790039) 	
Training Epoch: [3][700/979]	Time  (0.708546977580529)	Data (0.003244317377174802)	loss  (0.22594865761529895)	Prec1  (92.62482452392578) 	
Training Epoch: [3][800/979]	Time  (0.6938035928652379)	Data (0.002898134988791934)	loss  (0.22260133764728643)	Prec1  (92.87141418457031) 	
Training Epoch: [3][900/979]	Time  (0.6976752604019894)	Data (0.002627536274087548)	loss  (0.21961854504070943)	Prec1  (92.819091796875) 	
The current loss: 327
The Last loss:  304
trigger times: 2

******************************
	Adjusted learning rate: 4

0.0008145062499999999
Training Epoch: [4][0/979]	Time  (2.6887874603271484)	Data (2.0773379802703857)	loss  (0.024891918525099754)	Prec1  (100.0) 	
Training Epoch: [4][100/979]	Time  (0.6234049726240706)	Data (0.021104076121113088)	loss  (0.1531082163703176)	Prec1  (94.75247955322266) 	
Training Epoch: [4][200/979]	Time  (0.5706153532758874)	Data (0.01087211139166533)	loss  (0.1634579528327591)	Prec1  (94.5273666381836) 	
Training Epoch: [4][300/979]	Time  (0.5462784949331189)	Data (0.007409927456877952)	loss  (0.155425806959319)	Prec1  (94.91693878173828) 	
Training Epoch: [4][400/979]	Time  (0.5353727138547826)	Data (0.0056826568898417406)	loss  (0.15121733389015046)	Prec1  (95.21196746826172) 	
Training Epoch: [4][500/979]	Time  (0.5285869325230459)	Data (0.004635052290743221)	loss  (0.15012153877749826)	Prec1  (95.1696548461914) 	
Training Epoch: [4][600/979]	Time  (0.5238520789662138)	Data (0.003931719134135572)	loss  (0.1497328592244088)	Prec1  (95.22462463378906) 	
Training Epoch: [4][700/979]	Time  (0.5206697786415525)	Data (0.0034204900009655917)	loss  (0.1458078171833174)	Prec1  (95.2781753540039) 	
Training Epoch: [4][800/979]	Time  (0.522089920984523)	Data (0.003037849169098929)	loss  (0.1421033600166489)	Prec1  (95.4556884765625) 	
Training Epoch: [4][900/979]	Time  (0.5196063206277862)	Data (0.0027393090738175845)	loss  (0.1410557620216225)	Prec1  (95.50499725341797) 	
The current loss: 318
The Last loss:  327

******************************
	Adjusted learning rate: 5

0.0007737809374999998
Training Epoch: [5][0/979]	Time  (2.0221846103668213)	Data (1.5315678119659424)	loss  (1.1015188694000244)	Prec1  (90.0) 	
Training Epoch: [5][100/979]	Time  (0.5127888127128677)	Data (0.015531622537291876)	loss  (0.11004813411042537)	Prec1  (96.73267364501953) 	
Training Epoch: [5][200/979]	Time  (0.5064598683694109)	Data (0.007995177264237286)	loss  (0.11301669641658525)	Prec1  (96.31841278076172) 	
Training Epoch: [5][300/979]	Time  (0.5043145493415503)	Data (0.005464638586456198)	loss  (0.11420370494553114)	Prec1  (96.31228637695312) 	
Training Epoch: [5][400/979]	Time  (0.5031771796599885)	Data (0.0041905989373414)	loss  (0.10842911680818343)	Prec1  (96.40897369384766) 	
Training Epoch: [5][500/979]	Time  (0.5086776615378862)	Data (0.0034246516084956552)	loss  (0.10849228191750399)	Prec1  (96.40718078613281) 	
Training Epoch: [5][600/979]	Time  (0.507152282061077)	Data (0.002912862130290458)	loss  (0.10538921017304742)	Prec1  (96.5224609375) 	
Training Epoch: [5][700/979]	Time  (0.5058264228994939)	Data (0.0025467563117621797)	loss  (0.10367095199061196)	Prec1  (96.60485076904297) 	
Training Epoch: [5][800/979]	Time  (0.5045812019843436)	Data (0.0022735857636145738)	loss  (0.10282181159989355)	Prec1  (96.65419006347656) 	
Training Epoch: [5][900/979]	Time  (0.503872085490846)	Data (0.0020579227993676188)	loss  (0.09891450792922123)	Prec1  (96.81465148925781) 	
The current loss: 283
The Last loss:  318

******************************
	Adjusted learning rate: 6

0.0007350918906249997
Training Epoch: [6][0/979]	Time  (1.9492712020874023)	Data (1.4649512767791748)	loss  (0.22062072157859802)	Prec1  (90.0) 	
Training Epoch: [6][100/979]	Time  (0.5117433944551071)	Data (0.014867843967853206)	loss  (0.08548340689688276)	Prec1  (97.62376403808594) 	
Training Epoch: [6][200/979]	Time  (0.5218402046469314)	Data (0.007647408774836146)	loss  (0.07600578572235472)	Prec1  (97.76119995117188) 	
Training Epoch: [6][300/979]	Time  (0.5144306179692975)	Data (0.005221997384613138)	loss  (0.07423144564456735)	Prec1  (97.77408599853516) 	
Training Epoch: [6][400/979]	Time  (0.5107355956127518)	Data (0.0040079869534309365)	loss  (0.07764064933848262)	Prec1  (97.6059799194336) 	
Training Epoch: [6][500/979]	Time  (0.5089638147525445)	Data (0.003284187374000778)	loss  (0.07289145409274266)	Prec1  (97.78443145751953) 	
Training Epoch: [6][600/979]	Time  (0.5075607601299064)	Data (0.002799142418605912)	loss  (0.06875489025811285)	Prec1  (97.82029724121094) 	
Training Epoch: [6][700/979]	Time  (0.506508086105216)	Data (0.0024489727237935414)	loss  (0.06608473781904965)	Prec1  (97.83167266845703) 	
Training Epoch: [6][800/979]	Time  (0.5093196736144067)	Data (0.002184360661310203)	loss  (0.0674090168632732)	Prec1  (97.76529693603516) 	
Training Epoch: [6][900/979]	Time  (0.5079401947152734)	Data (0.001982267900524076)	loss  (0.06582572231597404)	Prec1  (97.84683990478516) 	
The current loss: 311
The Last loss:  283
trigger times: 3
Early stopping!
Start to test process.
Testing started
Testing Epoch: [6][0/490]	Time  (1.7047226428985596)	Data (1.4611361026763916)	loss  (0.007110956124961376)	Prec1  (100.0) 	
Testing Epoch: [6][100/490]	Time  (0.25533194353084754)	Data (0.01748033089212852)	loss  (0.9784069802003479)	Prec1  (75.64356994628906) 	
Testing Epoch: [6][200/490]	Time  (0.24889584323066977)	Data (0.010281412162590976)	loss  (0.9418646898027973)	Prec1  (76.31841278076172) 	
Testing Epoch: [6][300/490]	Time  (0.2466954487898817)	Data (0.007882262385168742)	loss  (0.9892770916218111)	Prec1  (76.14617919921875) 	
Testing Epoch: [6][400/490]	Time  (0.24462676107734813)	Data (0.006670796663089286)	loss  (0.9651449689913908)	Prec1  (76.35910034179688) 	
Testing Epoch: [6][489/490]	Time  (0.25015693732670374)	Data (0.006003887799321389)	loss  (0.9661489769976784)	Prec1  (75.7860336303711) 	
tensor([[349.,  43.,   8.,  21.,  46.,   1.,  26.],
        [ 50., 832.,   9.,  20.,  31.,   2.,  15.],
        [ 12.,  10.,  91.,   6.,   9.,   3.,   6.],
        [ 21.,   9.,   2., 210.,   5.,   5.,   4.],
        [ 47.,  22.,   8.,   7., 282.,   3.,  16.],
        [  3.,   4.,   5.,   8.,   8.,  29.,   1.],
        [ 31.,   8.,  22.,   8.,  28.,   0.,  63.]])
tensor([0.7065, 0.8676, 0.6642, 0.8203, 0.7325, 0.5000, 0.3938])
Epoch: 6   Test Acc: 75.7860336303711
