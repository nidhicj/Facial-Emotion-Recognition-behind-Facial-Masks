DB: A
Device state: cuda

FER on AffectNet using GACNN


Total included  26950 {0: 3984, 1: 3932, 2: 3946, 3: 4003, 4: 4019, 5: 4002, 6: 3064}
Total included  6674 {0: 984, 1: 1057, 2: 1001, 3: 986, 4: 959, 5: 972, 6: 715}
Total included  3488 {0: 499, 1: 500, 2: 497, 3: 499, 4: 496, 5: 500, 6: 497}
length of  train Database for training: 26950
length of  valid Database for validation training: 6674
length of  test Database: 3488
prepare model
+------------------------------------------+------------+
|                 Modules                  | Parameters |
+------------------------------------------+------------+
|           module.base.0.weight           |    1728    |
|            module.base.0.bias            |     64     |
|           module.base.2.weight           |   36864    |
|            module.base.2.bias            |     64     |
|           module.base.5.weight           |   73728    |
|            module.base.5.bias            |    128     |
|           module.base.7.weight           |   147456   |
|            module.base.7.bias            |    128     |
|          module.base.10.weight           |   294912   |
|           module.base.10.bias            |    256     |
|          module.base.12.weight           |   589824   |
|           module.base.12.bias            |    256     |
|          module.base.14.weight           |   589824   |
|           module.base.14.bias            |    256     |
|          module.base.17.weight           |  1179648   |
|           module.base.17.bias            |    512     |
|          module.base.19.weight           |  2359296   |
|           module.base.19.bias            |    512     |
| module.local_attention_block.0.1.weight  |   65536    |
|  module.local_attention_block.0.1.bias   |    128     |
| module.local_attention_block.0.3.weight  |    128     |
|  module.local_attention_block.0.3.bias   |    128     |
| module.local_attention_block.0.5.weight  |   73728    |
|  module.local_attention_block.0.5.bias   |     64     |
| module.local_attention_block.0.7.weight  |     64     |
|  module.local_attention_block.0.7.bias   |     1      |
| module.local_attention_block.1.1.weight  |   65536    |
|  module.local_attention_block.1.1.bias   |    128     |
| module.local_attention_block.1.3.weight  |    128     |
|  module.local_attention_block.1.3.bias   |    128     |
| module.local_attention_block.1.5.weight  |   73728    |
|  module.local_attention_block.1.5.bias   |     64     |
| module.local_attention_block.1.7.weight  |     64     |
|  module.local_attention_block.1.7.bias   |     1      |
| module.local_attention_block.2.1.weight  |   65536    |
|  module.local_attention_block.2.1.bias   |    128     |
| module.local_attention_block.2.3.weight  |    128     |
|  module.local_attention_block.2.3.bias   |    128     |
| module.local_attention_block.2.5.weight  |   73728    |
|  module.local_attention_block.2.5.bias   |     64     |
| module.local_attention_block.2.7.weight  |     64     |
|  module.local_attention_block.2.7.bias   |     1      |
| module.local_attention_block.3.1.weight  |   65536    |
|  module.local_attention_block.3.1.bias   |    128     |
| module.local_attention_block.3.3.weight  |    128     |
|  module.local_attention_block.3.3.bias   |    128     |
| module.local_attention_block.3.5.weight  |   73728    |
|  module.local_attention_block.3.5.bias   |     64     |
| module.local_attention_block.3.7.weight  |     64     |
|  module.local_attention_block.3.7.bias   |     1      |
| module.local_attention_block.4.1.weight  |   65536    |
|  module.local_attention_block.4.1.bias   |    128     |
| module.local_attention_block.4.3.weight  |    128     |
|  module.local_attention_block.4.3.bias   |    128     |
| module.local_attention_block.4.5.weight  |   73728    |
|  module.local_attention_block.4.5.bias   |     64     |
| module.local_attention_block.4.7.weight  |     64     |
|  module.local_attention_block.4.7.bias   |     1      |
| module.local_attention_block.5.1.weight  |   65536    |
|  module.local_attention_block.5.1.bias   |    128     |
| module.local_attention_block.5.3.weight  |    128     |
|  module.local_attention_block.5.3.bias   |    128     |
| module.local_attention_block.5.5.weight  |   73728    |
|  module.local_attention_block.5.5.bias   |     64     |
| module.local_attention_block.5.7.weight  |     64     |
|  module.local_attention_block.5.7.bias   |     1      |
| module.local_attention_block.6.1.weight  |   65536    |
|  module.local_attention_block.6.1.bias   |    128     |
| module.local_attention_block.6.3.weight  |    128     |
|  module.local_attention_block.6.3.bias   |    128     |
| module.local_attention_block.6.5.weight  |   73728    |
|  module.local_attention_block.6.5.bias   |     64     |
| module.local_attention_block.6.7.weight  |     64     |
|  module.local_attention_block.6.7.bias   |     1      |
| module.local_attention_block.7.1.weight  |   65536    |
|  module.local_attention_block.7.1.bias   |    128     |
| module.local_attention_block.7.3.weight  |    128     |
|  module.local_attention_block.7.3.bias   |    128     |
| module.local_attention_block.7.5.weight  |   73728    |
|  module.local_attention_block.7.5.bias   |     64     |
| module.local_attention_block.7.7.weight  |     64     |
|  module.local_attention_block.7.7.bias   |     1      |
| module.local_attention_block.8.1.weight  |   65536    |
|  module.local_attention_block.8.1.bias   |    128     |
| module.local_attention_block.8.3.weight  |    128     |
|  module.local_attention_block.8.3.bias   |    128     |
| module.local_attention_block.8.5.weight  |   73728    |
|  module.local_attention_block.8.5.bias   |     64     |
| module.local_attention_block.8.7.weight  |     64     |
|  module.local_attention_block.8.7.bias   |     1      |
| module.local_attention_block.9.1.weight  |   65536    |
|  module.local_attention_block.9.1.bias   |    128     |
| module.local_attention_block.9.3.weight  |    128     |
|  module.local_attention_block.9.3.bias   |    128     |
| module.local_attention_block.9.5.weight  |   73728    |
|  module.local_attention_block.9.5.bias   |     64     |
| module.local_attention_block.9.7.weight  |     64     |
|  module.local_attention_block.9.7.bias   |     1      |
| module.local_attention_block.10.1.weight |   65536    |
|  module.local_attention_block.10.1.bias  |    128     |
| module.local_attention_block.10.3.weight |    128     |
|  module.local_attention_block.10.3.bias  |    128     |
| module.local_attention_block.10.5.weight |   73728    |
|  module.local_attention_block.10.5.bias  |     64     |
| module.local_attention_block.10.7.weight |     64     |
|  module.local_attention_block.10.7.bias  |     1      |
| module.local_attention_block.11.1.weight |   65536    |
|  module.local_attention_block.11.1.bias  |    128     |
| module.local_attention_block.11.3.weight |    128     |
|  module.local_attention_block.11.3.bias  |    128     |
| module.local_attention_block.11.5.weight |   73728    |
|  module.local_attention_block.11.5.bias  |     64     |
| module.local_attention_block.11.7.weight |     64     |
|  module.local_attention_block.11.7.bias  |     1      |
| module.local_attention_block.12.1.weight |   65536    |
|  module.local_attention_block.12.1.bias  |    128     |
| module.local_attention_block.12.3.weight |    128     |
|  module.local_attention_block.12.3.bias  |    128     |
| module.local_attention_block.12.5.weight |   73728    |
|  module.local_attention_block.12.5.bias  |     64     |
| module.local_attention_block.12.7.weight |     64     |
|  module.local_attention_block.12.7.bias  |     1      |
| module.local_attention_block.13.1.weight |   65536    |
|  module.local_attention_block.13.1.bias  |    128     |
| module.local_attention_block.13.3.weight |    128     |
|  module.local_attention_block.13.3.bias  |    128     |
| module.local_attention_block.13.5.weight |   73728    |
|  module.local_attention_block.13.5.bias  |     64     |
| module.local_attention_block.13.7.weight |     64     |
|  module.local_attention_block.13.7.bias  |     1      |
| module.local_attention_block.14.1.weight |   65536    |
|  module.local_attention_block.14.1.bias  |    128     |
| module.local_attention_block.14.3.weight |    128     |
|  module.local_attention_block.14.3.bias  |    128     |
| module.local_attention_block.14.5.weight |   73728    |
|  module.local_attention_block.14.5.bias  |     64     |
| module.local_attention_block.14.7.weight |     64     |
|  module.local_attention_block.14.7.bias  |     1      |
| module.local_attention_block.15.1.weight |   65536    |
|  module.local_attention_block.15.1.bias  |    128     |
| module.local_attention_block.15.3.weight |    128     |
|  module.local_attention_block.15.3.bias  |    128     |
| module.local_attention_block.15.5.weight |   73728    |
|  module.local_attention_block.15.5.bias  |     64     |
| module.local_attention_block.15.7.weight |     64     |
|  module.local_attention_block.15.7.bias  |     1      |
| module.local_attention_block.16.1.weight |   65536    |
|  module.local_attention_block.16.1.bias  |    128     |
| module.local_attention_block.16.3.weight |    128     |
|  module.local_attention_block.16.3.bias  |    128     |
| module.local_attention_block.16.5.weight |   73728    |
|  module.local_attention_block.16.5.bias  |     64     |
| module.local_attention_block.16.7.weight |     64     |
|  module.local_attention_block.16.7.bias  |     1      |
| module.local_attention_block.17.1.weight |   65536    |
|  module.local_attention_block.17.1.bias  |    128     |
| module.local_attention_block.17.3.weight |    128     |
|  module.local_attention_block.17.3.bias  |    128     |
| module.local_attention_block.17.5.weight |   73728    |
|  module.local_attention_block.17.5.bias  |     64     |
| module.local_attention_block.17.7.weight |     64     |
|  module.local_attention_block.17.7.bias  |     1      |
| module.local_attention_block.18.1.weight |   65536    |
|  module.local_attention_block.18.1.bias  |    128     |
| module.local_attention_block.18.3.weight |    128     |
|  module.local_attention_block.18.3.bias  |    128     |
| module.local_attention_block.18.5.weight |   73728    |
|  module.local_attention_block.18.5.bias  |     64     |
| module.local_attention_block.18.7.weight |     64     |
|  module.local_attention_block.18.7.bias  |     1      |
| module.local_attention_block.19.1.weight |   65536    |
|  module.local_attention_block.19.1.bias  |    128     |
| module.local_attention_block.19.3.weight |    128     |
|  module.local_attention_block.19.3.bias  |    128     |
| module.local_attention_block.19.5.weight |   73728    |
|  module.local_attention_block.19.5.bias  |     64     |
| module.local_attention_block.19.7.weight |     64     |
|  module.local_attention_block.19.7.bias  |     1      |
|  module.global_attention_block.1.weight  |   65536    |
|   module.global_attention_block.1.bias   |    128     |
|  module.global_attention_block.3.weight  |    128     |
|   module.global_attention_block.3.bias   |    128     |
|  module.global_attention_block.5.weight  |   401408   |
|   module.global_attention_block.5.bias   |     64     |
|  module.global_attention_block.7.weight  |     64     |
|   module.global_attention_block.7.bias   |     1      |
|       module.PG_unit_1.0.0.weight        |  2359296   |
|        module.PG_unit_1.0.0.bias         |    512     |
|       module.PG_unit_1.1.0.weight        |  2359296   |
|        module.PG_unit_1.1.0.bias         |    512     |
|       module.PG_unit_1.2.0.weight        |  2359296   |
|        module.PG_unit_1.2.0.bias         |    512     |
|       module.PG_unit_1.3.0.weight        |  2359296   |
|        module.PG_unit_1.3.0.bias         |    512     |
|       module.PG_unit_1.4.0.weight        |  2359296   |
|        module.PG_unit_1.4.0.bias         |    512     |
|       module.PG_unit_1.5.0.weight        |  2359296   |
|        module.PG_unit_1.5.0.bias         |    512     |
|       module.PG_unit_1.6.0.weight        |  2359296   |
|        module.PG_unit_1.6.0.bias         |    512     |
|       module.PG_unit_1.7.0.weight        |  2359296   |
|        module.PG_unit_1.7.0.bias         |    512     |
|       module.PG_unit_1.8.0.weight        |  2359296   |
|        module.PG_unit_1.8.0.bias         |    512     |
|       module.PG_unit_1.9.0.weight        |  2359296   |
|        module.PG_unit_1.9.0.bias         |    512     |
|       module.PG_unit_1.10.0.weight       |  2359296   |
|        module.PG_unit_1.10.0.bias        |    512     |
|       module.PG_unit_1.11.0.weight       |  2359296   |
|        module.PG_unit_1.11.0.bias        |    512     |
|       module.PG_unit_1.12.0.weight       |  2359296   |
|        module.PG_unit_1.12.0.bias        |    512     |
|       module.PG_unit_1.13.0.weight       |  2359296   |
|        module.PG_unit_1.13.0.bias        |    512     |
|       module.PG_unit_1.14.0.weight       |  2359296   |
|        module.PG_unit_1.14.0.bias        |    512     |
|       module.PG_unit_1.15.0.weight       |  2359296   |
|        module.PG_unit_1.15.0.bias        |    512     |
|       module.PG_unit_1.16.0.weight       |  2359296   |
|        module.PG_unit_1.16.0.bias        |    512     |
|       module.PG_unit_1.17.0.weight       |  2359296   |
|        module.PG_unit_1.17.0.bias        |    512     |
|       module.PG_unit_1.18.0.weight       |  2359296   |
|        module.PG_unit_1.18.0.bias        |    512     |
|       module.PG_unit_1.19.0.weight       |  2359296   |
|        module.PG_unit_1.19.0.bias        |    512     |
|       module.PG_unit_2.0.1.weight        |  1179648   |
|        module.PG_unit_2.0.1.bias         |     64     |
|       module.PG_unit_2.1.1.weight        |  1179648   |
|        module.PG_unit_2.1.1.bias         |     64     |
|       module.PG_unit_2.2.1.weight        |  1179648   |
|        module.PG_unit_2.2.1.bias         |     64     |
|       module.PG_unit_2.3.1.weight        |  1179648   |
|        module.PG_unit_2.3.1.bias         |     64     |
|       module.PG_unit_2.4.1.weight        |  1179648   |
|        module.PG_unit_2.4.1.bias         |     64     |
|       module.PG_unit_2.5.1.weight        |  1179648   |
|        module.PG_unit_2.5.1.bias         |     64     |
|       module.PG_unit_2.6.1.weight        |  1179648   |
|        module.PG_unit_2.6.1.bias         |     64     |
|       module.PG_unit_2.7.1.weight        |  1179648   |
|        module.PG_unit_2.7.1.bias         |     64     |
|       module.PG_unit_2.8.1.weight        |  1179648   |
|        module.PG_unit_2.8.1.bias         |     64     |
|       module.PG_unit_2.9.1.weight        |  1179648   |
|        module.PG_unit_2.9.1.bias         |     64     |
|       module.PG_unit_2.10.1.weight       |  1179648   |
|        module.PG_unit_2.10.1.bias        |     64     |
|       module.PG_unit_2.11.1.weight       |  1179648   |
|        module.PG_unit_2.11.1.bias        |     64     |
|       module.PG_unit_2.12.1.weight       |  1179648   |
|        module.PG_unit_2.12.1.bias        |     64     |
|       module.PG_unit_2.13.1.weight       |  1179648   |
|        module.PG_unit_2.13.1.bias        |     64     |
|       module.PG_unit_2.14.1.weight       |  1179648   |
|        module.PG_unit_2.14.1.bias        |     64     |
|       module.PG_unit_2.15.1.weight       |  1179648   |
|        module.PG_unit_2.15.1.bias        |     64     |
|       module.PG_unit_2.16.1.weight       |  1179648   |
|        module.PG_unit_2.16.1.bias        |     64     |
|       module.PG_unit_2.17.1.weight       |  1179648   |
|        module.PG_unit_2.17.1.bias        |     64     |
|       module.PG_unit_2.18.1.weight       |  1179648   |
|        module.PG_unit_2.18.1.bias        |     64     |
|       module.PG_unit_2.19.1.weight       |  1179648   |
|        module.PG_unit_2.19.1.bias        |     64     |
|        module.GG_unit_1.1.weight         |  2359296   |
|         module.GG_unit_1.1.bias          |    512     |
|        module.GG_unit_2.1.weight         |  51380224  |
|         module.GG_unit_2.1.bias          |    512     |
|            module.fc1.weight             |   917504   |
|             module.fc1.bias              |    512     |
|            module.fc2.weight             |    3584    |
|             module.fc2.bias              |     7      |
+------------------------------------------+------------+
Total Trainable Params: 133991004

Training starting:

Training Epoch: [0][0/2695]	Time  (5.489854097366333)	Data (1.8868319988250732)	loss  (1.957278847694397)	Prec1  (10.0) 	
Training Epoch: [0][100/2695]	Time  (0.3569075353074782)	Data (0.019041030713827303)	loss  (1.9337974090387327)	Prec1  (20.099010467529297) 	
Training Epoch: [0][200/2695]	Time  (0.33416334906620765)	Data (0.009769145529068525)	loss  (1.8591184639812108)	Prec1  (24.278608322143555) 	
Training Epoch: [0][300/2695]	Time  (0.3290399237724634)	Data (0.00663689521460042)	loss  (1.8027164072689423)	Prec1  (26.478404998779297) 	
Training Epoch: [0][400/2695]	Time  (0.3261236383433354)	Data (0.005067888340747862)	loss  (1.749850757698763)	Prec1  (28.902742385864258) 	
Training Epoch: [0][500/2695]	Time  (0.3271127422888598)	Data (0.004122494223588955)	loss  (1.701081320435225)	Prec1  (31.556884765625) 	
Training Epoch: [0][600/2695]	Time  (0.32841128715857887)	Data (0.0034915778085515027)	loss  (1.6665297713930318)	Prec1  (33.22795486450195) 	
Training Epoch: [0][700/2695]	Time  (0.3298378732167025)	Data (0.0030414962904599524)	loss  (1.6316831066843107)	Prec1  (34.6505012512207) 	
Training Epoch: [0][800/2695]	Time  (0.32823325215505156)	Data (0.0027021424154217324)	loss  (1.6015856167052718)	Prec1  (35.86766815185547) 	
Training Epoch: [0][900/2695]	Time  (0.327289631575776)	Data (0.0024418510686808766)	loss  (1.5746953208226873)	Prec1  (37.11431884765625) 	
Training Epoch: [0][1000/2695]	Time  (0.3281476926374864)	Data (0.0022349807765934017)	loss  (1.5538894805041226)	Prec1  (37.982017517089844) 	
Training Epoch: [0][1100/2695]	Time  (0.32786950255176567)	Data (0.0020646726294715873)	loss  (1.5322551727294922)	Prec1  (39.155311584472656) 	
Training Epoch: [0][1200/2695]	Time  (0.3291211954064413)	Data (0.0019209627108609647)	loss  (1.5083519783643362)	Prec1  (40.29975128173828) 	
Training Epoch: [0][1300/2695]	Time  (0.3294183194499855)	Data (0.0017991652404409843)	loss  (1.4852188870505494)	Prec1  (41.44504165649414) 	
Training Epoch: [0][1400/2695]	Time  (0.33004134455210477)	Data (0.0016937439650999146)	loss  (1.46776956085815)	Prec1  (42.44111251831055) 	
Training Epoch: [0][1500/2695]	Time  (0.33023616093782326)	Data (0.0016031482869350933)	loss  (1.4490367605795786)	Prec1  (43.33111572265625) 	
Training Epoch: [0][1600/2695]	Time  (0.32982531046584423)	Data (0.0015239764719289367)	loss  (1.4356880314941334)	Prec1  (43.966270446777344) 	
Training Epoch: [0][1700/2695]	Time  (0.32514834053582264)	Data (0.0014558861635489578)	loss  (1.4232881166246762)	Prec1  (44.620811462402344) 	
Training Epoch: [0][1800/2695]	Time  (0.3198446494086063)	Data (0.0013910099772993954)	loss  (1.4105461031645288)	Prec1  (45.09716796875) 	
Training Epoch: [0][1900/2695]	Time  (0.31443561722516133)	Data (0.0013316669694879444)	loss  (1.3969940724559735)	Prec1  (45.71278381347656) 	
Training Epoch: [0][2000/2695]	Time  (0.3111649262553629)	Data (0.0012777816766741752)	loss  (1.38636535732285)	Prec1  (46.27186584472656) 	
Training Epoch: [0][2100/2695]	Time  (0.31280280509941466)	Data (0.0012299376746009272)	loss  (1.3743908838224888)	Prec1  (46.863399505615234) 	
Training Epoch: [0][2200/2695]	Time  (0.3133095410237362)	Data (0.0011890628889223381)	loss  (1.3646117224443073)	Prec1  (47.314857482910156) 	
Training Epoch: [0][2300/2695]	Time  (0.31435102807185483)	Data (0.0011525154113769531)	loss  (1.35418331490968)	Prec1  (47.818336486816406) 	
Training Epoch: [0][2400/2695]	Time  (0.3149838845762597)	Data (0.0011181498705869117)	loss  (1.3427647692444225)	Prec1  (48.321533203125) 	
Training Epoch: [0][2500/2695]	Time  (0.31523209436089455)	Data (0.0010866433417782788)	loss  (1.3318090309909896)	Prec1  (48.800479888916016) 	
Training Epoch: [0][2600/2695]	Time  (0.3158489689832465)	Data (0.0010567308159710488)	loss  (1.3223675325003004)	Prec1  (49.123416900634766) 	
The current loss: 781
The Last loss:  1000

******************************
	Adjusted learning rate: 1

0.00095
Training Epoch: [1][0/2695]	Time  (2.253600835800171)	Data (1.7486402988433838)	loss  (0.9270004034042358)	Prec1  (50.0) 	
Training Epoch: [1][100/2695]	Time  (0.37029682527674307)	Data (0.017639490637448754)	loss  (1.0429596632423968)	Prec1  (61.1881217956543) 	
Training Epoch: [1][200/2695]	Time  (0.3563576064892669)	Data (0.008975260293305809)	loss  (1.074711000296607)	Prec1  (60.14925765991211) 	
Training Epoch: [1][300/2695]	Time  (0.34934840645900994)	Data (0.0061024716526170905)	loss  (1.0698452964177558)	Prec1  (60.3986701965332) 	
Training Epoch: [1][400/2695]	Time  (0.3308362199778569)	Data (0.004663867546138621)	loss  (1.0685551924598484)	Prec1  (60.24937438964844) 	
Training Epoch: [1][500/2695]	Time  (0.30846125636985916)	Data (0.0037820596180990072)	loss  (1.0501474520403469)	Prec1  (60.97804260253906) 	
Training Epoch: [1][600/2695]	Time  (0.29482197642524705)	Data (0.0031930245892973787)	loss  (1.0452107645806774)	Prec1  (60.798667907714844) 	
Training Epoch: [1][700/2695]	Time  (0.29015997745171085)	Data (0.0027732995369294912)	loss  (1.0403463941242828)	Prec1  (60.984310150146484) 	
Training Epoch: [1][800/2695]	Time  (0.29399269290928837)	Data (0.0024662797668304633)	loss  (1.0348084900486336)	Prec1  (61.34831619262695) 	
Training Epoch: [1][900/2695]	Time  (0.2974081182321089)	Data (0.0022272738182584403)	loss  (1.0363069701604917)	Prec1  (61.32075500488281) 	
Training Epoch: [1][1000/2695]	Time  (0.3029533485313515)	Data (0.0020387451370041092)	loss  (1.0290960949974937)	Prec1  (61.728271484375) 	
Training Epoch: [1][1100/2695]	Time  (0.30638331873215513)	Data (0.001881810343341325)	loss  (1.0225830255964472)	Prec1  (61.99818420410156) 	
Training Epoch: [1][1200/2695]	Time  (0.3077551665452994)	Data (0.0017526133868418366)	loss  (1.0180845960391551)	Prec1  (62.36469650268555) 	
Training Epoch: [1][1300/2695]	Time  (0.30921429436908693)	Data (0.0016445233948316875)	loss  (1.0126169030021284)	Prec1  (62.65949249267578) 	
Training Epoch: [1][1400/2695]	Time  (0.3104406376552105)	Data (0.0015514337701681765)	loss  (1.0099268904504906)	Prec1  (62.80514144897461) 	
Training Epoch: [1][1500/2695]	Time  (0.31233895325327143)	Data (0.0014686487580362279)	loss  (1.0011598218646867)	Prec1  (63.21119689941406) 	
Training Epoch: [1][1600/2695]	Time  (0.3138277331119325)	Data (0.0013980311501554815)	loss  (0.998767239462205)	Prec1  (63.32292175292969) 	
Training Epoch: [1][1700/2695]	Time  (0.31581422681320703)	Data (0.0013346579550014813)	loss  (0.9965910636488653)	Prec1  (63.39212417602539) 	
Training Epoch: [1][1800/2695]	Time  (0.31691132261645855)	Data (0.0012790557081867496)	loss  (0.9893092693950255)	Prec1  (63.66463088989258) 	
Training Epoch: [1][1900/2695]	Time  (0.3172975053039743)	Data (0.0012291703081206231)	loss  (0.9858315674365664)	Prec1  (63.887428283691406) 	
Training Epoch: [1][2000/2695]	Time  (0.318520884106363)	Data (0.001185462094735408)	loss  (0.9822696671224963)	Prec1  (64.04297637939453) 	
Training Epoch: [1][2100/2695]	Time  (0.31822841488594444)	Data (0.0011441484058204007)	loss  (0.9784651228157127)	Prec1  (64.22655487060547) 	
Training Epoch: [1][2200/2695]	Time  (0.3189122702846848)	Data (0.0011087211355844556)	loss  (0.9745771343303994)	Prec1  (64.3480224609375) 	
Training Epoch: [1][2300/2695]	Time  (0.3167655075285446)	Data (0.0010724634463141556)	loss  (0.9698316499247648)	Prec1  (64.56757354736328) 	
Training Epoch: [1][2400/2695]	Time  (0.3132921050658379)	Data (0.0010378662619378257)	loss  (0.9682269141059376)	Prec1  (64.65222930908203) 	
Training Epoch: [1][2500/2695]	Time  (0.3101542064639293)	Data (0.0010064264051154442)	loss  (0.9654779724839305)	Prec1  (64.74610137939453) 	
Training Epoch: [1][2600/2695]	Time  (0.307647443101847)	Data (0.0009773983858219252)	loss  (0.9604534894651928)	Prec1  (64.89811706542969) 	
The current loss: 831
The Last loss:  781
trigger times: 1

******************************
	Adjusted learning rate: 2

0.0009025
Training Epoch: [2][0/2695]	Time  (2.5613105297088623)	Data (1.9583008289337158)	loss  (0.6211332082748413)	Prec1  (80.0) 	
Training Epoch: [2][100/2695]	Time  (0.36578908769210966)	Data (0.01976100525053421)	loss  (0.9361760132383592)	Prec1  (67.32673645019531) 	
Training Epoch: [2][200/2695]	Time  (0.34647275322112275)	Data (0.010116383804017632)	loss  (0.8844657108854892)	Prec1  (69.35323333740234) 	
Training Epoch: [2][300/2695]	Time  (0.342420552656104)	Data (0.006879278195656811)	loss  (0.8836354104387404)	Prec1  (68.87042999267578) 	
Training Epoch: [2][400/2695]	Time  (0.33996355801151873)	Data (0.005260598927067402)	loss  (0.8698607696559364)	Prec1  (69.2269287109375) 	
Training Epoch: [2][500/2695]	Time  (0.3385427440711838)	Data (0.004288068074666098)	loss  (0.8640661701232849)	Prec1  (69.64071655273438) 	
Training Epoch: [2][600/2695]	Time  (0.3392955943470985)	Data (0.0036371666659134596)	loss  (0.8504970616687356)	Prec1  (69.86688995361328) 	
Training Epoch: [2][700/2695]	Time  (0.33854067988810627)	Data (0.0031559093192368532)	loss  (0.8458318059012485)	Prec1  (69.98573303222656) 	
Training Epoch: [2][800/2695]	Time  (0.3368558446119787)	Data (0.002804724613527829)	loss  (0.8405312429392681)	Prec1  (69.97503662109375) 	
Training Epoch: [2][900/2695]	Time  (0.33712585546597257)	Data (0.002531567106765595)	loss  (0.8444245110837522)	Prec1  (69.7447280883789) 	
Training Epoch: [2][1000/2695]	Time  (0.3328742235452383)	Data (0.002312222203531942)	loss  (0.8417528972967522)	Prec1  (69.91008758544922) 	
Training Epoch: [2][1100/2695]	Time  (0.32358596067229367)	Data (0.0021265129085890713)	loss  (0.8353000353213119)	Prec1  (70.0181655883789) 	
Training Epoch: [2][1200/2695]	Time  (0.31601629626443245)	Data (0.0019675998465405417)	loss  (0.8345652843618472)	Prec1  (70.01665496826172) 	
Training Epoch: [2][1300/2695]	Time  (0.3086697338728791)	Data (0.0018364208464802091)	loss  (0.8281946033239365)	Prec1  (70.36125946044922) 	
Training Epoch: [2][1400/2695]	Time  (0.30977717167475155)	Data (0.0017237848762441414)	loss  (0.8252697189223502)	Prec1  (70.54960632324219) 	
Training Epoch: [2][1500/2695]	Time  (0.3103773361996442)	Data (0.001631632238765465)	loss  (0.8234840135309078)	Prec1  (70.65956115722656) 	
Training Epoch: [2][1600/2695]	Time  (0.311278781616859)	Data (0.0015510786033882938)	loss  (0.8188394711101002)	Prec1  (70.84322357177734) 	
Training Epoch: [2][1700/2695]	Time  (0.3129541632850194)	Data (0.001480384128083908)	loss  (0.816729507311031)	Prec1  (70.98765563964844) 	
Training Epoch: [2][1800/2695]	Time  (0.31334825915008835)	Data (0.001418439895824748)	loss  (0.8148699076323956)	Prec1  (71.0494155883789) 	
Training Epoch: [2][1900/2695]	Time  (0.3141764088218805)	Data (0.0013617937468278414)	loss  (0.8124849272206982)	Prec1  (71.20462799072266) 	
Training Epoch: [2][2000/2695]	Time  (0.31508918275599596)	Data (0.001308847700935909)	loss  (0.8113338595804485)	Prec1  (71.30435180664062) 	
Training Epoch: [2][2100/2695]	Time  (0.3160088328961132)	Data (0.0012644208765551909)	loss  (0.8064317338058689)	Prec1  (71.49928283691406) 	
Training Epoch: [2][2200/2695]	Time  (0.31681866128029795)	Data (0.0012230997679180474)	loss  (0.8035319114898563)	Prec1  (71.6810531616211) 	
Training Epoch: [2][2300/2695]	Time  (0.31731844311017465)	Data (0.0011861737734957499)	loss  (0.8022013624716406)	Prec1  (71.72533416748047) 	
Training Epoch: [2][2400/2695]	Time  (0.31743043941639204)	Data (0.0011516018541392462)	loss  (0.7990936180890873)	Prec1  (71.83257293701172) 	
Training Epoch: [2][2500/2695]	Time  (0.3179685442221732)	Data (0.0011196470127159098)	loss  (0.7952017191706634)	Prec1  (71.96321105957031) 	
Training Epoch: [2][2600/2695]	Time  (0.31799556978937754)	Data (0.0010903317027255142)	loss  (0.7946059822646621)	Prec1  (72.0184555053711) 	
The current loss: 804
The Last loss:  831

******************************
	Adjusted learning rate: 3

0.000857375
Training Epoch: [3][0/2695]	Time  (2.012284994125366)	Data (1.8067002296447754)	loss  (0.10287211835384369)	Prec1  (100.0) 	
Training Epoch: [3][100/2695]	Time  (0.32090076125494327)	Data (0.01811447001919888)	loss  (0.7258155158545712)	Prec1  (75.3465347290039) 	
Training Epoch: [3][200/2695]	Time  (0.32561385809485593)	Data (0.009270372675425971)	loss  (0.6894175425097717)	Prec1  (76.06965637207031) 	
Training Epoch: [3][300/2695]	Time  (0.3318276745932443)	Data (0.006297744389784296)	loss  (0.6785220447370777)	Prec1  (76.31228637695312) 	
Training Epoch: [3][400/2695]	Time  (0.32757792805792985)	Data (0.004809647724218202)	loss  (0.6807697750906695)	Prec1  (76.10972595214844) 	
Training Epoch: [3][500/2695]	Time  (0.3259608512391111)	Data (0.0039052501648010134)	loss  (0.6796717555104139)	Prec1  (76.24750518798828) 	
Training Epoch: [3][600/2695]	Time  (0.32626663428574754)	Data (0.003307581344579103)	loss  (0.6737976539353165)	Prec1  (76.28952026367188) 	
Training Epoch: [3][700/2695]	Time  (0.3255631923675537)	Data (0.002883887665077895)	loss  (0.6655811275569417)	Prec1  (76.67617797851562) 	
Training Epoch: [3][800/2695]	Time  (0.3258463526784109)	Data (0.0025510552819450845)	loss  (0.6671091481838631)	Prec1  (76.79151153564453) 	
Training Epoch: [3][900/2695]	Time  (0.32800524824864324)	Data (0.0023058500723886436)	loss  (0.6680807796132221)	Prec1  (76.67036437988281) 	
Training Epoch: [3][1000/2695]	Time  (0.327489239114386)	Data (0.002107563075962124)	loss  (0.667541141060742)	Prec1  (76.77322387695312) 	
Training Epoch: [3][1100/2695]	Time  (0.3275362624567709)	Data (0.0019458167451170767)	loss  (0.6645368090564289)	Prec1  (77.03905487060547) 	
Training Epoch: [3][1200/2695]	Time  (0.327999935856866)	Data (0.0018108403255897795)	loss  (0.6646526810789882)	Prec1  (76.92755889892578) 	
Training Epoch: [3][1300/2695]	Time  (0.3285057284481978)	Data (0.001692322563153794)	loss  (0.6632626461957529)	Prec1  (77.00230407714844) 	
Training Epoch: [3][1400/2695]	Time  (0.32834284327695573)	Data (0.0015946455634891093)	loss  (0.6633633045395896)	Prec1  (77.07351684570312) 	
Training Epoch: [3][1500/2695]	Time  (0.3301979491585179)	Data (0.0015078547158136437)	loss  (0.6617015873458606)	Prec1  (77.2418441772461) 	
Training Epoch: [3][1600/2695]	Time  (0.33091611224811274)	Data (0.0014344302659329588)	loss  (0.6588726362200694)	Prec1  (77.27045440673828) 	
Training Epoch: [3][1700/2695]	Time  (0.32419505265094056)	Data (0.001369288078411266)	loss  (0.6561967107932193)	Prec1  (77.46619415283203) 	
Training Epoch: [3][1800/2695]	Time  (0.31882230723188826)	Data (0.001304931074033374)	loss  (0.6536339373637543)	Prec1  (77.54580688476562) 	
Training Epoch: [3][1900/2695]	Time  (0.31387186150749)	Data (0.0012482960935268574)	loss  (0.6516986347417465)	Prec1  (77.6170425415039) 	
Training Epoch: [3][2000/2695]	Time  (0.30966546748769935)	Data (0.001197198579932141)	loss  (0.649019979923472)	Prec1  (77.70614624023438) 	
Training Epoch: [3][2100/2695]	Time  (0.30732973140515013)	Data (0.0011497492565534502)	loss  (0.6473585273334663)	Prec1  (77.72013092041016) 	
Training Epoch: [3][2200/2695]	Time  (0.30342893316658015)	Data (0.001107422561333538)	loss  (0.6439202226654392)	Prec1  (77.85098266601562) 	
Training Epoch: [3][2300/2695]	Time  (0.297192458327259)	Data (0.001068005298646001)	loss  (0.6426403219093193)	Prec1  (77.87918090820312) 	
Training Epoch: [3][2400/2695]	Time  (0.29155168161944317)	Data (0.001032251559014819)	loss  (0.6400248446237639)	Prec1  (77.96334838867188) 	
Training Epoch: [3][2500/2695]	Time  (0.28635832558913693)	Data (0.00099919834693686)	loss  (0.637379966217463)	Prec1  (78.11275482177734) 	
Training Epoch: [3][2600/2695]	Time  (0.28193770404597146)	Data (0.0009675561258857959)	loss  (0.6362478216993309)	Prec1  (78.18916320800781) 	
The current loss: 914
The Last loss:  804
trigger times: 2

******************************
	Adjusted learning rate: 4

0.0008145062499999999
Training Epoch: [4][0/2695]	Time  (1.9739315509796143)	Data (1.780320644378662)	loss  (0.5499900579452515)	Prec1  (80.0) 	
Training Epoch: [4][100/2695]	Time  (0.19286777949569248)	Data (0.01784899919339926)	loss  (0.6053793639829843)	Prec1  (77.82178497314453) 	
Training Epoch: [4][200/2695]	Time  (0.17566110720088826)	Data (0.009083470301841623)	loss  (0.5871018094580565)	Prec1  (79.35324096679688) 	
Training Epoch: [4][300/2695]	Time  (0.17010993656525977)	Data (0.006140573476240088)	loss  (0.5690634053856432)	Prec1  (79.9667739868164) 	
Training Epoch: [4][400/2695]	Time  (0.16760027795063884)	Data (0.00466626837961097)	loss  (0.5628794176211381)	Prec1  (80.42393493652344) 	
Training Epoch: [4][500/2695]	Time  (0.16783785724830247)	Data (0.003777108030642816)	loss  (0.54908204536714)	Prec1  (81.0578842163086) 	
Training Epoch: [4][600/2695]	Time  (0.16499224875413637)	Data (0.003182419921316442)	loss  (0.5459704101767496)	Prec1  (81.06488800048828) 	
Training Epoch: [4][700/2695]	Time  (0.1634934166188587)	Data (0.002761915304861463)	loss  (0.5476421323709838)	Prec1  (81.16976165771484) 	
Training Epoch: [4][800/2695]	Time  (0.16363428088460819)	Data (0.0024440922540671815)	loss  (0.5474925165793646)	Prec1  (81.2359619140625) 	
Training Epoch: [4][900/2695]	Time  (0.1623154744985498)	Data (0.002196948615612915)	loss  (0.5446372034274182)	Prec1  (81.53163146972656) 	
Training Epoch: [4][1000/2695]	Time  (0.16140691359917245)	Data (0.001999814312655728)	loss  (0.5440540964288133)	Prec1  (81.43856048583984) 	
Training Epoch: [4][1100/2695]	Time  (0.16112476257060898)	Data (0.0018410063352939977)	loss  (0.5383838104569674)	Prec1  (81.58038330078125) 	
Training Epoch: [4][1200/2695]	Time  (0.16156027279328944)	Data (0.0017046467846974446)	loss  (0.5345677926075597)	Prec1  (81.6985855102539) 	
Training Epoch: [4][1300/2695]	Time  (0.16085147839340222)	Data (0.0015902392778096063)	loss  (0.5351199618899346)	Prec1  (81.63719940185547) 	
Training Epoch: [4][1400/2695]	Time  (0.16043099465325933)	Data (0.0014927441694326353)	loss  (0.5295551625279169)	Prec1  (81.84867858886719) 	
Training Epoch: [4][1500/2695]	Time  (0.16035818195914842)	Data (0.0014088434985603674)	loss  (0.5318401103726353)	Prec1  (81.7988052368164) 	
Training Epoch: [4][1600/2695]	Time  (0.1605214770327204)	Data (0.0013343111415865419)	loss  (0.528374006835596)	Prec1  (81.90505981445312) 	
Training Epoch: [4][1700/2695]	Time  (0.16032068537376265)	Data (0.001268855408315866)	loss  (0.5251829462264966)	Prec1  (81.98118591308594) 	
Training Epoch: [4][1800/2695]	Time  (0.1600708097037443)	Data (0.0012116302456344783)	loss  (0.5218231471204744)	Prec1  (82.04886627197266) 	
Training Epoch: [4][1900/2695]	Time  (0.1599637750949689)	Data (0.0011595165898836267)	loss  (0.5187139006832253)	Prec1  (82.18305969238281) 	
Training Epoch: [4][2000/2695]	Time  (0.1601467098014942)	Data (0.001111987589121699)	loss  (0.5181075363982653)	Prec1  (82.19390106201172) 	
Training Epoch: [4][2100/2695]	Time  (0.15971509619362861)	Data (0.0010693046037836451)	loss  (0.5174367550897037)	Prec1  (82.2132339477539) 	
Training Epoch: [4][2200/2695]	Time  (0.1595798107452254)	Data (0.001031807691928529)	loss  (0.5149336607228865)	Prec1  (82.33985137939453) 	
Training Epoch: [4][2300/2695]	Time  (0.15999419447755048)	Data (0.0009965776412811345)	loss  (0.5139716175740592)	Prec1  (82.3598403930664) 	
Training Epoch: [4][2400/2695]	Time  (0.15956767734414784)	Data (0.0009636049814792237)	loss  (0.5132705842201807)	Prec1  (82.3698501586914) 	
Training Epoch: [4][2500/2695]	Time  (0.15944410247451923)	Data (0.0009337977760555934)	loss  (0.5114265456140303)	Prec1  (82.4110336303711) 	
Training Epoch: [4][2600/2695]	Time  (0.15922001718053996)	Data (0.0009061034392137245)	loss  (0.5099288597903495)	Prec1  (82.4913558959961) 	
The current loss: 963
The Last loss:  914
trigger times: 3
Early stopping!
Start to test process.
Testing started
Testing Epoch: [4][0/698]	Time  (1.7689063549041748)	Data (1.689500093460083)	loss  (2.388468027114868)	Prec1  (60.0) 	
Testing Epoch: [4][100/698]	Time  (0.0792815378396818)	Data (0.017755871952170193)	loss  (1.4020030449817675)	Prec1  (57.02970504760742) 	
Testing Epoch: [4][200/698]	Time  (0.07727819058432508)	Data (0.009260541764064809)	loss  (1.4269279192632705)	Prec1  (55.52239227294922) 	
Testing Epoch: [4][300/698]	Time  (0.07218813183299727)	Data (0.006490010359754594)	loss  (1.4597356207049963)	Prec1  (55.61461639404297) 	
Testing Epoch: [4][400/698]	Time  (0.06935599082128663)	Data (0.005081933038193091)	loss  (1.484377582601627)	Prec1  (55.61096954345703) 	
Testing Epoch: [4][500/698]	Time  (0.06803435955694812)	Data (0.00424677121663046)	loss  (1.4731979275683682)	Prec1  (56.247501373291016) 	
Testing Epoch: [4][600/698]	Time  (0.0684777964371413)	Data (0.003643773359784271)	loss  (1.4935762806425177)	Prec1  (55.77370834350586) 	
Testing Epoch: [4][697/698]	Time  (0.06730140314402758)	Data (0.0032176503479309955)	loss  (1.4955752867566627)	Prec1  (55.475914001464844) 	
tensor([[251.,  32.,  43.,  19., 112.,  17.,  25.],
        [ 41., 392.,   7.,  13.,  23.,   7.,  17.],
        [ 71.,   5., 217.,  14.,  87.,  31.,  72.],
        [ 67.,  37.,  16., 170.,  71., 111.,  27.],
        [ 53.,   6.,  31.,   8., 339.,  24.,  35.],
        [ 27.,   8.,  18.,  47.,  70., 301.,  29.],
        [ 35.,  21.,  61.,   6.,  81.,  28., 265.]])
tensor([0.5030, 0.7840, 0.4366, 0.3407, 0.6835, 0.6020, 0.5332])
Epoch: 4   Test Acc: 55.475914001464844
